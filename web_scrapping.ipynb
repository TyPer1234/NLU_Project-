{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /Users/alex/.pyenv/versions/3.10.7/lib/python3.10/site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/alex/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from bs4) (4.13.3)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/alex/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from beautifulsoup4->bs4) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/alex/.pyenv/versions/3.10.7/lib/python3.10/site-packages (from beautifulsoup4->bs4) (2.6)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/presidential-campaigns-debates-and-endorsements-0\"\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debate Date: September 10, 2024\n"
     ]
    }
   ],
   "source": [
    "# Extract the debate date\n",
    "date_td = soup.find(\"td\", style=lambda x: x and \"width:112pt\" in x)\n",
    "debate_date = date_td.get_text(strip=True) if date_td else \"No date found\"\n",
    "\n",
    "print(\"Debate Date:\", debate_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debate Name: Presidential Debate in Philadelphia, Pennsylvania(Harris-Trump)\n"
     ]
    }
   ],
   "source": [
    "# Extract the debate document name\n",
    "name_td = date_td.find_next(\"td\")  # Move to the next <td> in the row\n",
    "debate_name = name_td.get_text(strip=True) if name_td else \"No name found\"\n",
    "\n",
    "print(\"Debate Name:\", debate_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debate Link: https://www.presidency.ucsb.edu/documents/presidential-debate-philadelphia-pennsylvania\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Extract the hyperlink\n",
    "link_tag = name_td.find(\"a\")  # Find the <a> tag inside the <td>\n",
    "hyperlink = link_tag[\"href\"] if link_tag else \"No link found\"\n",
    "\n",
    "print(\"Debate Link:\", hyperlink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid debate date: (1) The second presidential debate, a \"town hall\" style format, scheduled for October 15, 2020 was cancelled on October 9, 2020.  The Commission on Presidential Debates modified the format of this debate stating that it would be held \"virtually\" because of concerns about public health due to President Trump's COVID-19 diagnosis.  The President then chose not to participate in this modified format.  Both Donald Trump and Joe Biden held \"town-hall\" events in lieu of the debate.  These event transcripts are below:President Trump:Remarks in a Town Hall Meeting with Savannah Guthrie of NBC News at the Perez Art Museum in Miami, FloridaFormer Vice President Biden:Remarks in a Town Hall Meeting with George Stephanopoulos of ABC News at the National Constitution Center in Philadelphia, Pennsylvania\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/presidential-campaigns-debates-and-endorsements-0\"\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Store links in a structured format\n",
    "link_dict = {}\n",
    "for link in soup.find_all('tr'):\n",
    "    # Extracting the date\n",
    "    date_td = link.find('td',style=lambda x: x and \"width:112pt\" in x)\n",
    "    # Dealing with errors\n",
    "    if not date_td:\n",
    "        continue\n",
    "\n",
    "    debate_date = date_td.get_text(strip=True)\n",
    "\n",
    "    if len(debate_date) > 30:\n",
    "        print(f\"invalid debate date: {debate_date}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "    # Extracting the name of the debate document\n",
    "    name_td = date_td.find_next(\"td\")\n",
    "    if not name_td:\n",
    "        print(\"debugging: No name td found \")\n",
    "        continue\n",
    "    debate_name = name_td.get_text(strip=True)\n",
    "\n",
    "    #setting up some keywords that trucates our output\n",
    "    invalid_words = ['cancelled']\n",
    "    if any(word in debate_name.lower() for word in invalid_words):\n",
    "        continue\n",
    "\n",
    "    # Extracting the hyperlink\n",
    "    link_tag = name_td.find(\"a\")\n",
    "    if not link_tag:\n",
    "        print(\"debugging: No url found \")\n",
    "        continue\n",
    "    hyperlink = link_tag[\"href\"]\n",
    "\n",
    "    # Store in dictionary\n",
    "    link_dict[debate_name] = [debate_date, hyperlink]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting filename\n",
    "\n",
    "def format_filename(debate_name,debate_date):\n",
    "    \"\"\"Formating the filename to split correctly the debates\"\"\"\n",
    "    try:\n",
    "        date_obj = datetime.strptime(debate_date, \"%B %d, %Y\")\n",
    "        formatted_date = date_obj.strftime(\"%m_%d_%Y\")\n",
    "    except ValueError:\n",
    "        print(f\"invalid date: {debate_date}\")\n",
    "\n",
    "\n",
    "    if \"Vice Presidential\" in debate_name:\n",
    "        suffix = \"vp\"\n",
    "    elif \"Republican\" in debate_name:\n",
    "        suffix = \"rpd\"\n",
    "    elif \"Democratic\" in debate_name:\n",
    "        suffix = \"dcd\"\n",
    "    else:\n",
    "        suffix = \"pb\"\n",
    "    # returns structure\n",
    "    filename = f\"{formatted_date}_{suffix}.txt\"\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"data\"\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: 09_10_2024_pb.txt\n",
      "Filename: 06_27_2024_pb.txt\n",
      "Filename: 10_01_2024_vp.txt\n",
      "Filename: 12_10_2011_rpd.txt\n",
      "Filename: 12_06_2023_rpd.txt\n",
      "Filename: 12_09_2007_rpd.txt\n",
      "Filename: 05_03_2007_rpd.txt\n",
      "Filename: 11_10_2015_rpd.txt\n",
      "Filename: 10_22_2020_pb.txt\n",
      "Filename: 09_29_2020_pb.txt\n",
      "Filename: 10_07_2020_vp.txt\n",
      "Filename: 03_15_2020_dcd.txt\n",
      "Filename: 07_23_2007_dcd.txt\n",
      "Filename: 11_15_2007_dcd.txt\n",
      "Filename: 01_26_2000_dcd.txt\n",
      "Filename: 01_08_2000_dcd.txt\n",
      "Filename: 03_01_2000_dcd.txt\n",
      "Filename: 11_20_2019_dcd.txt\n",
      "Filename: 10_15_2019_dcd.txt\n",
      "Filename: 09_12_2019_dcd.txt\n",
      "Filename: 07_31_2019_dcd.txt\n",
      "Filename: 07_30_2019_dcd.txt\n",
      "Filename: 06_27_2019_dcd.txt\n",
      "Filename: 06_26_2019_dcd.txt\n",
      "Filename: 10_19_2016_pb.txt\n",
      "Filename: 10_09_2016_pb.txt\n",
      "Filename: 10_16_2012_pb.txt\n",
      "Filename: 10_04_2016_vp.txt\n",
      "Filename: 04_14_2016_dcd.txt\n",
      "Filename: 09_09_2007_dcd.txt\n",
      "Filename: 03_06_2016_dcd.txt\n",
      "Filename: 02_11_2016_dcd.txt\n",
      "Filename: 01_05_2000_dcd.txt\n",
      "Filename: 01_25_2016_dcd.txt\n",
      "Filename: 03_03_2016_rpd.txt\n",
      "Filename: 02_25_2016_rpd.txt\n",
      "Filename: 02_13_2016_rpd.txt\n",
      "Filename: 12_02_1999_rpd.txt\n",
      "Filename: 12_13_1999_rpd.txt\n",
      "Filename: 01_14_2016_rpd.txt\n",
      "Filename: 10_18_2011_rpd.txt\n",
      "Filename: 10_28_2015_rpd.txt\n",
      "Filename: 08_06_2015_rpd.txt\n",
      "Filename: 01_28_2016_rpd.txt\n",
      "Filename: 01_14_2016_rpd.txt\n",
      "Filename: 12_15_2015_rpd.txt\n",
      "Filename: 11_10_2015_rpd.txt\n",
      "Filename: 10_28_2015_rpd.txt\n",
      "Filename: 09_16_2015_rpd.txt\n",
      "Filename: 08_06_2015_rpd.txt\n",
      "Filename: 10_22_2012_pb.txt\n",
      "Filename: 10_03_2012_pb.txt\n",
      "Filename: 10_11_2012_vp.txt\n",
      "Filename: 02_22_2012_rpd.txt\n",
      "Filename: 01_26_2012_rpd.txt\n",
      "Filename: 09_12_2011_rpd.txt\n",
      "Filename: 01_19_2012_rpd.txt\n",
      "Filename: 01_10_2008_rpd.txt\n",
      "Filename: 01_08_2012_rpd.txt\n",
      "Filename: 12_15_2011_rpd.txt\n",
      "Filename: 11_22_2011_rpd.txt\n",
      "Filename: 11_12_2011_rpd.txt\n",
      "Filename: 11_09_2011_rpd.txt\n",
      "Filename: 10_11_2011_rpd.txt\n",
      "Filename: 10_21_2007_rpd.txt\n",
      "Filename: 09_05_2011_pb.txt\n",
      "Filename: 08_11_2011_rpd.txt\n",
      "Filename: 10_15_2008_pb.txt\n",
      "Filename: 10_07_2008_pb.txt\n",
      "Filename: 09_26_2008_pb.txt\n",
      "Filename: 10_02_2008_vp.txt\n",
      "Filename: 10_30_2007_dcd.txt\n",
      "Filename: 02_26_2008_dcd.txt\n",
      "Filename: 02_21_2008_dcd.txt\n",
      "Filename: 01_21_2008_dcd.txt\n",
      "Filename: 12_13_2007_dcd.txt\n",
      "Filename: 12_04_2007_dcd.txt\n",
      "Filename: 09_26_2007_dcd.txt\n",
      "Filename: 08_19_2007_dcd.txt\n",
      "Filename: 08_07_2007_dcd.txt\n",
      "Filename: 06_28_2007_dcd.txt\n",
      "Filename: 04_26_2007_dcd.txt\n",
      "Filename: 01_24_2008_rpd.txt\n",
      "Filename: 01_06_2008_rpd.txt\n",
      "Filename: 01_15_2000_rpd.txt\n",
      "Filename: 11_28_2007_rpd.txt\n",
      "Filename: 10_09_2007_rpd.txt\n",
      "Filename: 09_27_2007_rpd.txt\n",
      "Filename: 01_06_2000_rpd.txt\n",
      "Filename: 01_07_2000_rpd.txt\n",
      "Filename: 10_13_2004_pb.txt\n",
      "Filename: 10_08_2004_pb.txt\n",
      "Filename: 09_30_2004_pb.txt\n",
      "Filename: 10_05_2004_vp.txt\n",
      "Filename: 01_29_2004_dcd.txt\n",
      "Filename: 10_11_1992_pb.txt\n",
      "Filename: 09_25_1988_pb.txt\n",
      "Filename: 10_03_2000_pb.txt\n",
      "Filename: 10_05_2000_pb.txt\n",
      "Filename: 02_21_2000_dcd.txt\n",
      "Filename: 01_17_2000_dcd.txt\n",
      "Filename: 12_19_1999_dcd.txt\n",
      "Filename: 12_17_1999_dcd.txt\n",
      "Filename: 10_27_1999_dcd.txt\n",
      "Filename: 03_02_2000_rpd.txt\n",
      "Filename: 01_10_2000_rpd.txt\n",
      "Filename: 12_06_1999_rpd.txt\n",
      "Filename: 11_21_1999_rpd.txt\n",
      "Filename: 10_28_1999_rpd.txt\n",
      "Filename: 10_22_1999_rpd.txt\n",
      "Filename: 10_16_1996_pb.txt\n",
      "Filename: 10_06_1996_pb.txt\n",
      "Filename: 10_09_1996_vp.txt\n",
      "Filename: 10_19_1992_pb.txt\n",
      "Filename: 10_15_1992_pb.txt\n",
      "Filename: 10_13_1992_vp.txt\n",
      "Filename: 10_13_1988_pb.txt\n",
      "Filename: 10_05_1988_vp.txt\n",
      "Filename: 10_21_1984_pb.txt\n",
      "Filename: 10_07_1984_pb.txt\n",
      "Filename: 10_11_1984_vp.txt\n",
      "Filename: 10_28_1980_pb.txt\n",
      "Filename: 09_21_1980_pb.txt\n",
      "Filename: 10_22_1976_pb.txt\n",
      "Filename: 10_06_1976_pb.txt\n",
      "Filename: 09_23_1976_pb.txt\n",
      "Filename: 10_15_1976_vp.txt\n",
      "Filename: 10_21_1960_pb.txt\n",
      "Filename: 10_13_1960_pb.txt\n",
      "Filename: 10_07_1960_pb.txt\n",
      "Filename: 09_26_1960_pb.txt\n"
     ]
    }
   ],
   "source": [
    "for title, list in link_dict.items():\n",
    "    date, url = list\n",
    "    # Fetching\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Parsing HTML\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Finding body transcript\n",
    "    content_div = soup.find(\"div\", class_=\"field-docs-content\")\n",
    "    if not content_div:\n",
    "        continue\n",
    "\n",
    "    # Extract all paragraphs inside the content div\n",
    "    paragraphs = content_div.find_all(\"p\")\n",
    "    text_content = \"\\n\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
    "\n",
    "    # Saving debates as .txt files\n",
    "    file_name = os.path.join(save_dir, format_filename(title, date))\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
